{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8e215-ef33-4a13-844b-a306946e5637",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tracking_output = pd.read_csv('code_output_data_microns.csv')\n",
    "ground_truth = pd.read_csv('consensus_ground_truth_microns.csv')\n",
    "\n",
    "# Ground truth and predicted data as DataFrames\n",
    "gt_df = pd.DataFrame({'FrameId': ground_truth['frame'], 'Id': ground_truth['particle'], 'X': ground_truth['X'], 'Y': ground_truth['Y']})\n",
    "pred_df = pd.DataFrame({'FrameId': tracking_output['time'], 'Id': tracking_output['track_id'], 'X': tracking_output['X'], 'Y': tracking_output['Y']})\n",
    "\n",
    "# Initialize accumulator for metrics\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "for frame in sorted(set(gt_df['FrameId']).union(pred_df['FrameId'])):\n",
    "    gt_positions = gt_df[gt_df['FrameId'] == frame][['X', 'Y']].values\n",
    "    pred_positions = pred_df[pred_df['FrameId'] == frame][['X', 'Y']].values\n",
    "    acc.update(gt_df[gt_df['FrameId'] == frame]['Id'].values,\n",
    "               pred_df[pred_df['FrameId'] == frame]['Id'].values,\n",
    "               mm.distances.norm2squared_matrix(gt_positions, pred_positions, max_d2=10000))\n",
    "\n",
    "# Compute metrics\n",
    "metrics = ['idf1', 'idp', 'idr', 'recall', 'precision', 'num_false_positives', 'num_misses', 'num_switches', 'mota', 'motp']\n",
    "summary = mm.metrics.create().compute(acc, metrics=metrics, name='Overall')\n",
    "print(summary)\n",
    "\n",
    "# Compute IoU (Intersection over Union)\n",
    "iou_scores = []\n",
    "tracked_ids = set(pred_df['Id'])\n",
    "ground_truth_ids = set(gt_df['Id'])\n",
    "for obj_id in tracked_ids & ground_truth_ids:\n",
    "    track_tracked = set(pred_df[pred_df['Id'] == obj_id]['FrameId'])\n",
    "    track_gt = set(gt_df[gt_df['Id'] == obj_id]['FrameId'])\n",
    "    intersection = len(track_tracked & track_gt)\n",
    "    union = len(track_tracked | track_gt)\n",
    "    iou_scores.append(intersection / union if union > 0 else 0)\n",
    "mean_iou = np.mean(iou_scores) if iou_scores else None\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\" if mean_iou else \"No overlapping tracks for IoU calculation.\")\n",
    "\n",
    "# Visualize trajectories\n",
    "plt.figure(figsize=(10, 6))\n",
    "for obj_id in gt_df['Id'].unique():\n",
    "    plt.plot(gt_df[gt_df['Id'] == obj_id]['X'], gt_df[gt_df['Id'] == obj_id]['Y'], label=f\"GT {obj_id}\")\n",
    "for obj_id in pred_df['Id'].unique():\n",
    "    plt.plot(pred_df[pred_df['Id'] == obj_id]['X'], pred_df[pred_df['Id'] == obj_id]['Y'], linestyle='--', label=f\"Pred {obj_id}\")\n",
    "plt.title('Ground Truth vs Predicted Trajectories')\n",
    "plt.xlabel('X'); plt.ylabel('Y')\n",
    "plt.legend(); plt.grid(True); plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
